{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a1b8c3dce7afe3",
   "metadata": {},
   "source": [
    "# 4. Powerful Baseline Model\n",
    "\n",
    "Mocniejsza wersja baseline z większym modelem i lepszym treningiem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Importy i konfiguracja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "45c92880f31536be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: Metal Apple (MPS)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import constriction\n",
    "import os\n",
    "import struct\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Configuration\n",
    "IS_COLAB = False  # Set to True if running on Google Colab\n",
    "\n",
    "if IS_COLAB:\n",
    "    TRAIN_PATH = \"/content/all_silesia.bin\"\n",
    "    TEST_PATH = \"/content/all_canterbury.bin\"\n",
    "    COMPRESSED_PATH = \"/content/compressed_powerful.bin\"\n",
    "    DECOMPRESSED_PATH = \"/content/decompressed_powerful.txt\"\n",
    "    MODEL_PATH = \"/content/model_compressor_powerful.pth\"\n",
    "else:\n",
    "    TRAIN_PATH = \"../data/all_silesia.bin\" \n",
    "    TEST_PATH = \"../data/all_canterbury.bin\"\n",
    "    COMPRESSED_PATH = \"../out/compressed_powerful.bin\"\n",
    "    DECOMPRESSED_PATH = \"../out/decompressed_powerful.txt\"\n",
    "    MODEL_PATH = \"../out/model_compressor_powerful.pth\"\n",
    "\n",
    "# Model Hyperparameters\n",
    "EMBED_DIM = 128\n",
    "HIDDEN_SIZE = 512\n",
    "NUM_LAYERS = 3\n",
    "DROPOUT = 0.1\n",
    "\n",
    "# Training Hyperparameters\n",
    "EPOCHS = 2\n",
    "SEQ_LEN = 512\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-3\n",
    "CLIP_GRAD = 1.0\n",
    "\n",
    "# if metal apple is availble,  use it\n",
    "if torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device(\"mps\")\n",
    "    print(\"Using device: Metal Apple (MPS)\")\n",
    "\n",
    "# Device setup\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(\"Using device: CUDA\")\n",
    "# else:\n",
    "#     DEVICE = torch.device(\"cpu\")\n",
    "#     print(\"Using device: CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64b2a1ba2cf5136",
   "metadata": {},
   "source": [
    "## 2) DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "398fa82fe4b80fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ByteDataset(Dataset):\n",
    "    def __init__(self, file_path, seq_len):\n",
    "        if not os.path.exists(file_path):\n",
    "            # Create dummy data if file doesn't exist for testing flow\n",
    "            print(f\"Warning: {file_path} not found. Using dummy data.\")\n",
    "            self.data = torch.randint(0, 256, (100000,), dtype=torch.long)\n",
    "        else:\n",
    "            with open(file_path, 'rb') as f:\n",
    "                self.data = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "            self.data = torch.from_numpy(self.data).long()\n",
    "            \n",
    "        self.seq_len = seq_len\n",
    "        self.n_samples = len(self.data) - seq_len - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        # Ensure we don't return negative length\n",
    "        return max(0, self.n_samples // self.seq_len)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Use simple strided access. \n",
    "        # Ideally, for stateful RNNs, we want contiguous batches, but that requires custom sampler.\n",
    "        # Here we just rely on long SEQ_LEN to warm up the state.\n",
    "        start = idx * self.seq_len\n",
    "        end = start + self.seq_len + 1\n",
    "        \n",
    "        chunk = self.data[start:end]\n",
    "        return chunk[:-1], chunk[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d26b530c3503485",
   "metadata": {},
   "source": [
    "## 3) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6653084bb59f2a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PowerfulCompressor(nn.Module):\n",
    "    def __init__(self, vocab_size=257, embed_dim=EMBED_DIM, hidden_size=HIDDEN_SIZE, num_layers=NUM_LAYERS, dropout=DROPOUT):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.gru = nn.GRU(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size, 256) # Output 256 byte probabilities\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embed(x)\n",
    "        out, hidden = self.gru(x, hidden)\n",
    "        logits = self.fc(out)\n",
    "        return logits, hidden\n",
    "    \n",
    "    def _get_probs(self, x, hidden):\n",
    "        # Helper for inference\n",
    "        with torch.no_grad():\n",
    "            logits, hidden = self(x, hidden)\n",
    "            # Softmax over the last dimension\n",
    "            probs = torch.softmax(logits[0, 0], dim=0).cpu().numpy().astype(np.float64)\n",
    "            # Float64 helps with precision for arithmetic coding, though model is float32 usually\n",
    "        return probs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37082f587a27ca7a",
   "metadata": {},
   "source": [
    "## 4) Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45f3f8e2cf18fa9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_path, epochs=EPOCHS):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=1e-5)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=1, factor=0.5)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = torch.amp.GradScaler('cuda') if DEVICE.type == 'cuda' else None\n",
    "\n",
    "    dataset = ByteDataset(train_path, SEQ_LEN)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = []\n",
    "\n",
    "    print(f\"Training on {len(dataset)} sequences of length {SEQ_LEN}...\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = 0\n",
    "        steps = 0\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\", unit=\"batch\")\n",
    "\n",
    "        for x, y in pbar:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Mixed Precision Context\n",
    "            if DEVICE.type == 'cuda':\n",
    "                with torch.amp.autocast('cuda'):\n",
    "                    logits, _ = model(x)\n",
    "                    loss = criterion(logits.view(-1, 256), y.view(-1))\n",
    "                scaler.scale(loss).backward()\n",
    "                \n",
    "                # Gradient Clipping (unscale first)\n",
    "                scaler.unscale_(optimizer)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD)\n",
    "                \n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "            else:\n",
    "                # Standard Precision (CPU/MPS)\n",
    "                logits, _ = model(x)\n",
    "                loss = criterion(logits.view(-1, 256), y.view(-1))\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_GRAD)\n",
    "                optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            steps += 1\n",
    "            pbar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "        avg_loss = total_loss / steps\n",
    "        bpc = avg_loss / 0.693147\n",
    "        history.append({'loss': avg_loss, 'bpc': bpc})\n",
    "        \n",
    "        print(f\"Epoch {epoch + 1}/{epochs} | Loss: {avg_loss:.4f} | BPC: {bpc:.4f}\")\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Training finished in {total_time:.2f} seconds.\")\n",
    "    return history, total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Funkcje kompresji i dekompresji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a5d55a9fe058547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_time_profile(name, timings, total_time):\n",
    "    print(f\"\\n{name} time profile:\")\n",
    "    if total_time <= 0:\n",
    "        print(\"  No timing data\")\n",
    "        return\n",
    "\n",
    "    items = sorted(timings.items(), key=lambda x: x[1], reverse=True)\n",
    "    tracked = 0.0\n",
    "    for key, value in items:\n",
    "        tracked += value\n",
    "        pct = (value / total_time) * 100\n",
    "        print(f\"  {key:<16} {value:>8.4f}s  ({pct:>6.2f}%)\")\n",
    "\n",
    "    remaining = max(total_time - tracked, 0.0)\n",
    "    if remaining > 1e-9:\n",
    "        pct = (remaining / total_time) * 100\n",
    "        print(f\"  {'other':<16} {remaining:>8.4f}s  ({pct:>6.2f}%)\")\n",
    "\n",
    "\n",
    "def compress_file(model, input_path, output_path):\n",
    "    model.eval()\n",
    "    encoder = constriction.stream.queue.RangeEncoder()\n",
    "\n",
    "    timings = {\n",
    "        'read_bytes': 0.0,\n",
    "        'model_infer': 0.0,\n",
    "        'arith_encode': 0.0,\n",
    "        'state_update': 0.0,\n",
    "        'write_file': 0.0,\n",
    "    }\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        data_to_compress = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    timings['read_bytes'] += time.perf_counter() - t0\n",
    "\n",
    "    curr_symbol = torch.tensor([[256]], dtype=torch.long, device=DEVICE)\n",
    "    hidden = None\n",
    "    length = len(data_to_compress)\n",
    "\n",
    "    print(f\"Compressing {length} bytes...\")\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    for symbol in tqdm(data_to_compress, desc=\"Encoding\"):\n",
    "        t_model = time.perf_counter()\n",
    "        probs, hidden = model._get_probs(curr_symbol, hidden)\n",
    "        timings['model_infer'] += time.perf_counter() - t_model\n",
    "\n",
    "        probs = np.clip(probs, 1e-9, 1.0)\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        t_coder = time.perf_counter()\n",
    "        dist = constriction.stream.model.Categorical(probs, perfect=False)\n",
    "        encoder.encode(int(symbol), dist)\n",
    "        timings['arith_encode'] += time.perf_counter() - t_coder\n",
    "\n",
    "        t_state = time.perf_counter()\n",
    "        curr_symbol = torch.tensor([[symbol]], dtype=torch.long, device=DEVICE)\n",
    "        timings['state_update'] += time.perf_counter() - t_state\n",
    "\n",
    "    compressed_bits = encoder.get_compressed()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(struct.pack('<I', length))\n",
    "        f.write(compressed_bits.tobytes())\n",
    "    timings['write_file'] += time.perf_counter() - t0\n",
    "\n",
    "    duration = time.perf_counter() - start_time\n",
    "    original_size = length\n",
    "    compressed_size = os.path.getsize(output_path)\n",
    "    ratio = original_size / compressed_size\n",
    "    bpc = (compressed_size * 8) / original_size\n",
    "\n",
    "    print_time_profile(\"Compression\", timings, duration)\n",
    "\n",
    "    return {\n",
    "        'time': duration,\n",
    "        'original_size': original_size,\n",
    "        'compressed_size': compressed_size,\n",
    "        'ratio': ratio,\n",
    "        'bpc': bpc,\n",
    "        'speed_bps': original_size / duration,\n",
    "        'timings': timings,\n",
    "    }\n",
    "\n",
    "\n",
    "def decompress_file(model, input_path, output_path):\n",
    "    model.eval()\n",
    "\n",
    "    timings = {\n",
    "        'read_file': 0.0,\n",
    "        'model_infer': 0.0,\n",
    "        'arith_decode': 0.0,\n",
    "        'state_update': 0.0,\n",
    "        'write_file': 0.0,\n",
    "    }\n",
    "\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        orig_len = struct.unpack('<I', f.read(4))[0]\n",
    "        bits = np.frombuffer(f.read(), dtype=np.uint32)\n",
    "    timings['read_file'] += time.perf_counter() - t0\n",
    "\n",
    "    decoder = constriction.stream.queue.RangeDecoder(bits)\n",
    "    decoded_data = []\n",
    "    curr_symbol = torch.tensor([[256]], dtype=torch.long, device=DEVICE)\n",
    "    hidden = None\n",
    "\n",
    "    print(f\"Decompressing {orig_len} bytes...\")\n",
    "\n",
    "    for _ in tqdm(range(orig_len), desc=\"Decoding\"):\n",
    "        t_model = time.perf_counter()\n",
    "        probs, hidden = model._get_probs(curr_symbol, hidden)\n",
    "        timings['model_infer'] += time.perf_counter() - t_model\n",
    "\n",
    "        probs = np.clip(probs, 1e-9, 1.0)\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        t_coder = time.perf_counter()\n",
    "        dist = constriction.stream.model.Categorical(probs, perfect=False)\n",
    "        symbol = decoder.decode(dist)\n",
    "        timings['arith_decode'] += time.perf_counter() - t_coder\n",
    "\n",
    "        decoded_data.append(symbol)\n",
    "\n",
    "        t_state = time.perf_counter()\n",
    "        curr_symbol = torch.tensor([[symbol]], dtype=torch.long, device=DEVICE)\n",
    "        timings['state_update'] += time.perf_counter() - t_state\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(bytes(decoded_data))\n",
    "    timings['write_file'] += time.perf_counter() - t0\n",
    "\n",
    "    duration = time.perf_counter() - start_time\n",
    "\n",
    "    print_time_profile(\"Decompression\", timings, duration)\n",
    "\n",
    "    return {\n",
    "        'time': duration,\n",
    "        'speed_bps': orig_len / duration,\n",
    "        'timings': timings,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55420bd96c48eb32",
   "metadata": {},
   "source": [
    "## 6) Train i test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d16a647094764d89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Parameters: 4,302,208\n",
      "=== COMPRESSION ===\n",
      "Compressing 10846 bytes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 10846/10846 [00:07<00:00, 1419.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compression time profile:\n",
      "  model_infer        6.0512s  ( 79.20%)\n",
      "  state_update       1.4888s  ( 19.48%)\n",
      "  arith_encode       0.0150s  (  0.20%)\n",
      "  write_file         0.0004s  (  0.01%)\n",
      "  read_bytes         0.0002s  (  0.00%)\n",
      "  other              0.0850s  (  1.11%)\n",
      "Compression Ratio: 2.89x\n",
      "BPC: 2.77\n",
      "=== DECOMPRESSION ===\n",
      "Decompressing 10846 bytes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding: 100%|██████████| 10846/10846 [00:07<00:00, 1432.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decompression time profile:\n",
      "  model_infer        6.0064s  ( 79.32%)\n",
      "  state_update       1.4609s  ( 19.29%)\n",
      "  arith_decode       0.0150s  (  0.20%)\n",
      "  write_file         0.0003s  (  0.00%)\n",
      "  read_file          0.0001s  (  0.00%)\n",
      "  other              0.0898s  (  1.19%)\n",
      "SUCCESS: Integrity verified!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "TEST_PATH = \"../data/canterbury_small.bin\"\n",
    "model = PowerfulCompressor().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=\"cpu\"))\n",
    "model.eval()\n",
    "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "# # 1. Train\n",
    "# if os.path.exists(TRAIN_PATH):\n",
    "#     print(\"=== TRAINING ===\")\n",
    "#     train_metrics, train_time = train_model(model, TRAIN_PATH)\n",
    "#     torch.save(model.state_dict(), MODEL_PATH)\n",
    "#     print(f\"Model saved to {MODEL_PATH}\")\n",
    "# else:\n",
    "#     print(f\"Skipping training: {TRAIN_PATH} not found.\")\n",
    "\n",
    "# 2. Compress\n",
    "if os.path.exists(TEST_PATH):\n",
    "    print(\"=== COMPRESSION ===\")\n",
    "    comp_metrics = compress_file(model, TEST_PATH, COMPRESSED_PATH)\n",
    "    print(f\"Compression Ratio: {comp_metrics['ratio']:.2f}x\")\n",
    "    print(f\"BPC: {comp_metrics['bpc']:.2f}\")\n",
    "else:\n",
    "    print(f\"Skipping compression: {TEST_PATH} not found.\")\n",
    "\n",
    "# 3. Decompress\n",
    "if os.path.exists(COMPRESSED_PATH):\n",
    "    print(\"=== DECOMPRESSION ===\")\n",
    "    decomp_metrics = decompress_file(model, COMPRESSED_PATH, DECOMPRESSED_PATH)\n",
    "    \n",
    "    # Verify\n",
    "    with open(TEST_PATH, 'rb') as f1, open(DECOMPRESSED_PATH, 'rb') as f2:\n",
    "        if f1.read() == f2.read():\n",
    "            print(\"SUCCESS: Integrity verified!\")\n",
    "        else:\n",
    "            print(\"FAILURE: Data mismatch!\")\n",
    "else:\n",
    "    print(\"Skipping decompression: File not found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) Podsumowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Results:\n",
      "Compression Speed: 1419.52 B/s\n",
      "Decompression Speed: 1432.30 B/s\n",
      "Compression Ratio: 2.89x\n",
      "BPC: 2.77\n"
     ]
    }
   ],
   "source": [
    "print(f\"Baseline Results:\")\n",
    "print(f\"Compression Speed: {comp_metrics['speed_bps']:.2f} B/s\")\n",
    "print(f\"Decompression Speed: {decomp_metrics['speed_bps']:.2f} B/s\")\n",
    "print(f\"Compression Ratio: {comp_metrics['ratio']:.2f}x\")\n",
    "print(f\"BPC: {comp_metrics['bpc']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-compressor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
