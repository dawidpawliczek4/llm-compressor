{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b5a1b8c3dce7afe3",
   "metadata": {},
   "source": [
    "# 1. Baseline Model\n",
    "\n",
    "Podstawowy model LSTM -- baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89aaed01",
   "metadata": {},
   "source": [
    "## 1) Importy i konfiguracja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "45c92880f31536be",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T15:18:29.875873Z",
     "start_time": "2026-01-23T15:18:25.663316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import constriction\n",
    "import os\n",
    "import struct\n",
    "import time\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Konfiguracja\n",
    "TRAIN_PATH = \"../data/all_silesia.bin\"  # Zakładamy, że uruchamiamy z src/notebooks\n",
    "TEST_PATH = \"../data/all_canterbury.bin\"\n",
    "COMPRESSED_PATH = \"../out/compressed_baseline.bin\"\n",
    "DECOMPRESSED_PATH = \"../out/decompressed_baseline.txt\"\n",
    "MODEL_PATH = \"../out/model_compressor_baseline.pth\"\n",
    "\n",
    "HIDDEN_SIZE = 128\n",
    "EPOCHS = 1\n",
    "SEQ_LEN = 128\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a64b2a1ba2cf5136",
   "metadata": {},
   "source": [
    "## 2) DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "398fa82fe4b80fb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T15:18:29.917688Z",
     "start_time": "2026-01-23T15:18:29.911748Z"
    }
   },
   "outputs": [],
   "source": [
    "class ByteDataset(Dataset):\n",
    "    def __init__(self, file_path, seq_len):\n",
    "        with open(file_path, 'rb') as f:\n",
    "            self.data = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "\n",
    "        self.data = torch.from_numpy(self.data).long()\n",
    "        self.seq_len = seq_len\n",
    "        self.n_samples = len(self.data) - seq_len - 1\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples // SEQ_LEN\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start = idx * SEQ_LEN\n",
    "        end = start + SEQ_LEN + 1\n",
    "        \n",
    "        if end > len(self.data):\n",
    "            chunk = self.data[start:]\n",
    "            return chunk[:-1], chunk[1:]\n",
    "        \n",
    "        chunk = self.data[start:end]\n",
    "        return chunk[:-1], chunk[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d26b530c3503485",
   "metadata": {},
   "source": [
    "## 3) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6653084bb59f2a0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T15:18:29.984929Z",
     "start_time": "2026-01-23T15:18:29.976367Z"
    }
   },
   "outputs": [],
   "source": [
    "class Compressor(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Słownik 256 bajtów + 1 znak specjalny START\n",
    "        self.embed = nn.Embedding(257, 32)\n",
    "        self.lstm = nn.LSTM(32, HIDDEN_SIZE, batch_first=True)\n",
    "        self.fc = nn.Linear(HIDDEN_SIZE, 256)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.embed(x)\n",
    "        out, hidden = self.lstm(x, hidden)\n",
    "        logits = self.fc(out)\n",
    "        return logits, hidden\n",
    "    \n",
    "    def _get_probs(self, x, hidden):\n",
    "        # Helper do pobierania prawdopodobieństw pojedynczego kroku\n",
    "        with torch.no_grad():\n",
    "            logits, hidden = self(x, hidden)\n",
    "            probs = torch.softmax(logits[0, 0], dim=0).cpu().numpy().astype(np.float32)\n",
    "        return probs, hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37082f587a27ca7a",
   "metadata": {},
   "source": [
    "## 4) Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "45f3f8e2cf18fa9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T15:18:30.020530Z",
     "start_time": "2026-01-23T15:18:30.009084Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_model(model, train_path, epochs=EPOCHS):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    dataset = ByteDataset(train_path, SEQ_LEN)\n",
    "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, drop_last=True)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    history = []\n",
    "\n",
    "    for i in range(epochs):\n",
    "        total_loss = 0\n",
    "        steps = 0\n",
    "        pbar = tqdm(dataloader, desc=f\"Epoch {i + 1}/{epochs}\", unit=\"batch\")\n",
    "\n",
    "        for x, y in pbar:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(x)\n",
    "            loss = criterion(logits.view(-1, 256), y.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            steps += 1\n",
    "            pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "        avg_loss = total_loss / steps\n",
    "        bpc = avg_loss / 0.693147\n",
    "        history.append({'loss': avg_loss, 'bpc': bpc})\n",
    "        print(f\"Epoch {i + 1}/{epochs} | Loss: {avg_loss:.4f} | BPC: {bpc:.4f}\")\n",
    "\n",
    "    total_time = time.time() - start_time\n",
    "    print(f\"Training finished in {total_time:.2f} seconds.\")\n",
    "    return history, total_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d783e2f3e329f",
   "metadata": {},
   "source": [
    "## 5) Funkcje kompresji i dekompresji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a5d55a9fe058547",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T15:18:30.111637Z",
     "start_time": "2026-01-23T15:18:30.045862Z"
    }
   },
   "outputs": [],
   "source": [
    "def compress_file(model, input_path, output_path):\n",
    "    model.eval()\n",
    "    encoder = constriction.stream.queue.RangeEncoder()\n",
    "    \n",
    "    with open(input_path, \"rb\") as f:\n",
    "        data_to_compress = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    \n",
    "    curr_symbol = torch.tensor([[256]], dtype=torch.long, device=DEVICE)\n",
    "    hidden = None\n",
    "    length = len(data_to_compress)\n",
    "    \n",
    "    print(f\"Compressing {length} bytes...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Baseline: Symbol-by-symbol processing\n",
    "    for i, symbol in enumerate(tqdm(data_to_compress, desc=\"Encoding\")):\n",
    "        probs, hidden = model._get_probs(curr_symbol, hidden)\n",
    "        dist = constriction.stream.model.Categorical(probs, perfect=False)\n",
    "        encoder.encode(int(symbol), dist)\n",
    "        curr_symbol = torch.tensor([[symbol]], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    compressed_bits = encoder.get_compressed()\n",
    "    \n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(struct.pack('<I', length))\n",
    "        f.write(compressed_bits.tobytes())\n",
    "        \n",
    "    duration = time.time() - start_time\n",
    "    original_size = length\n",
    "    compressed_size = os.path.getsize(output_path)\n",
    "    ratio = original_size / compressed_size\n",
    "    bpc = (compressed_size * 8) / original_size\n",
    "    \n",
    "    print(f\"Compression finished in {duration:.2f}s\")\n",
    "    print(f\"Original size: {original_size} B\")\n",
    "    print(f\"Compressed size: {compressed_size} B\")\n",
    "    print(f\"Compression Ratio: {ratio:.2f}x\")\n",
    "    print(f\"Bits Per Character (BPC): {bpc:.2f}\")\n",
    "    \n",
    "    return {\n",
    "        'time': duration,\n",
    "        'original_size': original_size,\n",
    "        'compressed_size': compressed_size,\n",
    "        'ratio': ratio,\n",
    "        'bpc': bpc,\n",
    "        'speed_bps': original_size / duration\n",
    "    }\n",
    "\n",
    "def decompress_file(model, input_path, output_path):\n",
    "    model.eval()\n",
    "    \n",
    "    start_time = time.time()\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        orig_len = struct.unpack('<I', f.read(4))[0]\n",
    "        bits = np.frombuffer(f.read(), dtype=np.uint32)\n",
    "\n",
    "    decoder = constriction.stream.queue.RangeDecoder(bits)\n",
    "    decoded_data = []\n",
    "    curr_symbol = torch.tensor([[256]], dtype=torch.long, device=DEVICE)\n",
    "    hidden = None\n",
    "\n",
    "    print(f\"Decompressing {orig_len} bytes...\")\n",
    "    \n",
    "    for _ in tqdm(range(orig_len), desc=\"Decoding\"):\n",
    "        probs, hidden = model._get_probs(curr_symbol, hidden)\n",
    "        dist = constriction.stream.model.Categorical(probs, perfect=False)\n",
    "        symbol = decoder.decode(dist)\n",
    "        decoded_data.append(symbol)\n",
    "        curr_symbol = torch.tensor([[symbol]], dtype=torch.long, device=DEVICE)\n",
    "\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(bytes(decoded_data))\n",
    "        \n",
    "    duration = time.time() - start_time\n",
    "    print(f\"Decompression finished in {duration:.2f}s\")\n",
    "    \n",
    "    return {\n",
    "        'time': duration,\n",
    "        'speed_bps': orig_len / duration\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55420bd96c48eb32",
   "metadata": {},
   "source": [
    "## 6) Train i test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d16a647094764d89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T15:24:47.411141Z",
     "start_time": "2026-01-23T15:18:30.121079Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPRESSION ===\n",
      "Compressing 10846 bytes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding: 100%|██████████| 10846/10846 [00:00<00:00, 25085.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compression finished in 0.44s\n",
      "Original size: 10846 B\n",
      "Compressed size: 6112 B\n",
      "Compression Ratio: 1.77x\n",
      "Bits Per Character (BPC): 4.51\n",
      "=== DECOMPRESSION ===\n",
      "Decompressing 10846 bytes...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decoding: 100%|██████████| 10846/10846 [00:00<00:00, 24144.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decompression finished in 0.45s\n",
      "SUCCESS: Decompressed data matches original!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Inicjalizacja modelu\n",
    "model = Compressor().to(DEVICE)\n",
    "model.load_state_dict(torch.load(MODEL_PATH, map_location=DEVICE))\n",
    "model.eval()\n",
    "\n",
    "TEST_PATH = \"../data/canterbury_small.bin\"\n",
    "\n",
    "# 1. Trening\n",
    "# print(\"=== TRAINING ===\")\n",
    "# train_metrics, train_time = train_model(model, TRAIN_PATH)\n",
    "\n",
    "# Zapis modelu\n",
    "# torch.save(model.state_dict(), MODEL_PATH)\n",
    "\n",
    "# 2. Kompresja\n",
    "print(\"=== COMPRESSION ===\")\n",
    "# Uwaga: Dla testu na szybko można użyć mniejszego pliku tekstowego jeśli bible.txt jest duży\n",
    "# Tutaj używamy bible.txt zgodnie z oryginałem\n",
    "comp_metrics = compress_file(model, TEST_PATH, COMPRESSED_PATH)\n",
    "\n",
    "# 3. Dekompresja\n",
    "print(\"=== DECOMPRESSION ===\")\n",
    "decomp_metrics = decompress_file(model, COMPRESSED_PATH, DECOMPRESSED_PATH)\n",
    "\n",
    "# 4. Weryfikacja poprawności\n",
    "with open(TEST_PATH, 'rb') as f1, open(DECOMPRESSED_PATH, 'rb') as f2:\n",
    "    original = f1.read()\n",
    "    decompressed = f2.read()\n",
    "    if original == decompressed:\n",
    "        print(\"SUCCESS: Decompressed data matches original!\")\n",
    "    else:\n",
    "        print(\"FAILURE: Data mismatch!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "125e3117176325c4",
   "metadata": {},
   "source": [
    "## 7) Podsumowanie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce1e9bef2095d8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-23T15:24:47.489440Z",
     "start_time": "2026-01-23T15:24:47.487598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Results:\n",
      "Compression Speed: 24500.38 B/s\n",
      "Decompression Speed: 24030.46 B/s\n",
      "Compression Ratio: 1.77x\n",
      "BPC: 4.51\n"
     ]
    }
   ],
   "source": [
    "print(f\"Baseline Results:\")\n",
    "# print(f\"Training Time: {train_time:.2f}s\")\n",
    "print(f\"Compression Speed: {comp_metrics['speed_bps']:.2f} B/s\")\n",
    "print(f\"Decompression Speed: {decomp_metrics['speed_bps']:.2f} B/s\")\n",
    "print(f\"Compression Ratio: {comp_metrics['ratio']:.2f}x\")\n",
    "print(f\"BPC: {comp_metrics['bpc']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-compressor (3.12.12)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
