{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02f2cadb",
   "metadata": {},
   "source": [
    "# 5. GPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02095f06",
   "metadata": {},
   "source": [
    "## 1) Importy i konfiguracja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3a2616f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dawidpawliczek/Developer/llm-compressor/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import struct\n",
    "import time\n",
    "import constriction\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "TEST_PATH = \"../data/canterbury_small.bin\"\n",
    "COMPRESSED_PATH = \"../out/compressed_gpt2.bin\"\n",
    "DECOMPRESSED_PATH = \"../out/decompressed_gpt2.bin\"\n",
    "\n",
    "CONTEXT_SIZE = 1024\n",
    "STRIDE = 512\n",
    "MODEL_NAME = \"gpt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8d38d2",
   "metadata": {},
   "source": [
    "## 2) DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e786096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name=MODEL_NAME):\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Loaded: {params:,} parameters on {DEVICE}\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43db08c0",
   "metadata": {},
   "source": [
    "## 3) Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65a945c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_next_probs(model, input_ids, past_kv=None):\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids, past_key_values=past_kv, use_cache=True)\n",
    "        # Batch, token, vocab\n",
    "        logits = out.logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1).cpu().numpy().astype(np.float64)[0]\n",
    "        probs = np.clip(probs, 1e-9, 1.0)\n",
    "        probs /= probs.sum()\n",
    "    return probs, out.past_key_values\n",
    "\n",
    "\n",
    "def _reset_context(model, context_tokens):\n",
    "    input_ids = torch.tensor([context_tokens], device=DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids, use_cache=True)\n",
    "        logits = out.logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1).cpu().numpy().astype(np.float64)[0]\n",
    "        probs = np.clip(probs, 1e-9, 1.0)\n",
    "        probs /= probs.sum()\n",
    "    return probs, out.past_key_values, len(context_tokens)\n",
    "\n",
    "\n",
    "def print_time_profile(name, timings, total_time):\n",
    "    print(f\"\\n{name} time profile:\")\n",
    "    if total_time <= 0:\n",
    "        print(\"  No timing data\")\n",
    "        return\n",
    "\n",
    "    items = sorted(timings.items(), key=lambda x: x[1], reverse=True)\n",
    "    tracked = 0.0\n",
    "    for key, value in items:\n",
    "        tracked += value\n",
    "        pct = (value / total_time) * 100\n",
    "        print(f\"  {key:<16} {value:>8.4f}s  ({pct:>6.2f}%)\")\n",
    "\n",
    "    remaining = max(total_time - tracked, 0.0)\n",
    "    if remaining > 1e-9:\n",
    "        pct = (remaining / total_time) * 100\n",
    "        print(f\"  {'other':<16} {remaining:>8.4f}s  ({pct:>6.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf6e116",
   "metadata": {},
   "source": [
    "## 4) Trening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5721d265",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_file(model, tokenizer, input_path, output_path):\n",
    "    model.eval()\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    timings = {\n",
    "        \"read_bytes\": 0.0,\n",
    "        \"tokenize\": 0.0,\n",
    "        \"model_infer\": 0.0,\n",
    "        \"arith_encode\": 0.0,\n",
    "        \"write_file\": 0.0,\n",
    "    }\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        raw_bytes = f.read()\n",
    "    timings[\"read_bytes\"] += time.perf_counter() - t0\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    text = raw_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "    tokens = tokenizer.encode(text)\n",
    "    timings[\"tokenize\"] += time.perf_counter() - t0\n",
    "\n",
    "    num_tokens = len(tokens)\n",
    "    original_size = len(raw_bytes)\n",
    "\n",
    "    if tokenizer.decode(tokens).encode(\"utf-8\") != raw_bytes:\n",
    "        print(\"⚠️ Tokenization roundtrip not perfect - minor differences possible\")\n",
    "\n",
    "    print(f\"Original: {original_size:,} bytes → {num_tokens:,} tokens\")\n",
    "\n",
    "    encoder = constriction.stream.queue.RangeEncoder()\n",
    "    bos_id = tokenizer.eos_token_id\n",
    "\n",
    "    past_kv = None\n",
    "    kv_len = 0\n",
    "    all_seen = [bos_id]\n",
    "\n",
    "    for i in tqdm(range(num_tokens), desc=\"Compressing\"):\n",
    "        t_model = time.perf_counter()\n",
    "        if kv_len >= CONTEXT_SIZE:\n",
    "            context = all_seen[-STRIDE:]\n",
    "            probs, past_kv, kv_len = _reset_context(model, context)\n",
    "        else:\n",
    "            input_ids = torch.tensor([[all_seen[-1]]], device=DEVICE)\n",
    "            probs, past_kv = _get_next_probs(model, input_ids, past_kv)\n",
    "            kv_len += 1\n",
    "        timings[\"model_infer\"] += time.perf_counter() - t_model\n",
    "\n",
    "        t_coder = time.perf_counter()\n",
    "        dist = constriction.stream.model.Categorical(probs, perfect=False)\n",
    "        encoder.encode(int(tokens[i]), dist)\n",
    "        timings[\"arith_encode\"] += time.perf_counter() - t_coder\n",
    "\n",
    "        all_seen.append(tokens[i])\n",
    "\n",
    "    compressed_bits = encoder.get_compressed()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(struct.pack(\"<I\", original_size))\n",
    "        f.write(struct.pack(\"<I\", num_tokens))\n",
    "        f.write(compressed_bits.tobytes())\n",
    "    timings[\"write_file\"] += time.perf_counter() - t0\n",
    "\n",
    "    duration = time.perf_counter() - start_time\n",
    "    compressed_size = os.path.getsize(output_path)\n",
    "\n",
    "    print_time_profile(\"Compression\", timings, duration)\n",
    "\n",
    "    return {\n",
    "        \"time\": duration,\n",
    "        \"original_size\": original_size,\n",
    "        \"compressed_size\": compressed_size,\n",
    "        \"ratio\": original_size / compressed_size,\n",
    "        \"bpc\": (compressed_size * 8) / original_size,\n",
    "        \"speed_bps\": original_size / duration,\n",
    "        \"num_tokens\": num_tokens,\n",
    "        \"timings\": timings,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c930e946",
   "metadata": {},
   "source": [
    "## 5) Funkcje kompresji i dekompresji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d0e5c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress_file(model, tokenizer, input_path, output_path):\n",
    "    model.eval()\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    timings = {\n",
    "        \"read_file\": 0.0,\n",
    "        \"model_infer\": 0.0,\n",
    "        \"arith_decode\": 0.0,\n",
    "        \"detokenize\": 0.0,\n",
    "        \"write_file\": 0.0,\n",
    "    }\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        original_size = struct.unpack(\"<I\", f.read(4))[0]\n",
    "        num_tokens = struct.unpack(\"<I\", f.read(4))[0]\n",
    "        bits = np.frombuffer(f.read(), dtype=np.uint32)\n",
    "    timings[\"read_file\"] += time.perf_counter() - t0\n",
    "\n",
    "    decoder = constriction.stream.queue.RangeDecoder(bits)\n",
    "    bos_id = tokenizer.eos_token_id\n",
    "\n",
    "    print(f\"Decompressing: {num_tokens:,} tokens → {original_size:,} bytes\")\n",
    "\n",
    "    past_kv = None\n",
    "    kv_len = 0\n",
    "    all_seen = [bos_id]\n",
    "    decoded_tokens = []\n",
    "\n",
    "    for _ in tqdm(range(num_tokens), desc=\"Decompressing\"):\n",
    "        t_model = time.perf_counter()\n",
    "        if kv_len >= CONTEXT_SIZE:\n",
    "            context = all_seen[-STRIDE:]\n",
    "            probs, past_kv, kv_len = _reset_context(model, context)\n",
    "        else:\n",
    "            input_ids = torch.tensor([[all_seen[-1]]], device=DEVICE)\n",
    "            probs, past_kv = _get_next_probs(model, input_ids, past_kv)\n",
    "            kv_len += 1\n",
    "        timings[\"model_infer\"] += time.perf_counter() - t_model\n",
    "\n",
    "        t_coder = time.perf_counter()\n",
    "        dist = constriction.stream.model.Categorical(probs, perfect=False)\n",
    "        token = int(decoder.decode(dist))\n",
    "        timings[\"arith_decode\"] += time.perf_counter() - t_coder\n",
    "\n",
    "        decoded_tokens.append(token)\n",
    "        all_seen.append(token)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    text = tokenizer.decode(decoded_tokens)\n",
    "    timings[\"detokenize\"] += time.perf_counter() - t0\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(text.encode(\"utf-8\"))\n",
    "    timings[\"write_file\"] += time.perf_counter() - t0\n",
    "\n",
    "    duration = time.perf_counter() - start_time\n",
    "    print_time_profile(\"Decompression\", timings, duration)\n",
    "\n",
    "    return {\n",
    "        \"time\": duration,\n",
    "        \"speed_bps\": original_size / duration,\n",
    "        \"timings\": timings,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "462711f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_roundtrip(original_path, decoded_path):\n",
    "    with open(original_path, \"rb\") as f1, open(decoded_path, \"rb\") as f2:\n",
    "        orig_data = f1.read()\n",
    "        dec_data = f2.read()\n",
    "\n",
    "    if orig_data == dec_data:\n",
    "        print(\"✅ SUCCESS: Perfect match!\")\n",
    "        return True\n",
    "\n",
    "    print(\"❌ MISMATCH!\")\n",
    "    print(f\"   Original: {len(orig_data)} bytes, Decompressed: {len(dec_data)} bytes\")\n",
    "    for i in range(min(len(orig_data), len(dec_data))):\n",
    "        if orig_data[i] != dec_data[i]:\n",
    "            print(f\"   First diff at byte {i}: {orig_data[i]} vs {dec_data[i]}\")\n",
    "            break\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a73ca52",
   "metadata": {},
   "source": [
    "## 6) Train i test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74b16dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test file: ../data/canterbury_small.bin\n",
      "Loading gpt2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 148/148 [00:00<00:00, 3017.63it/s, Materializing param=transformer.wte.weight]             \n",
      "\u001b[1mGPT2LMHeadModel LOAD REPORT\u001b[0m from: gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "\u001b[3mNotes:\n",
      "- UNEXPECTED\u001b[3m\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\u001b[0m\n",
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3064 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 124,439,808 parameters on cpu\n",
      "\n",
      "=== COMPRESSION ===\n",
      "Original: 10,846 bytes → 3,064 tokens\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Compressing: 100%|██████████| 3064/3064 [00:25<00:00, 119.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compression time profile:\n",
      "  model_infer       25.3544s  ( 98.78%)\n",
      "  arith_encode       0.2091s  (  0.81%)\n",
      "  tokenize           0.0067s  (  0.03%)\n",
      "  write_file         0.0010s  (  0.00%)\n",
      "  read_bytes         0.0003s  (  0.00%)\n",
      "  other              0.0953s  (  0.37%)\n",
      "Ratio: 4.75x | BPC: 1.68\n",
      "\n",
      "=== DECOMPRESSION ===\n",
      "Decompressing: 3,064 tokens → 10,846 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Decompressing: 100%|██████████| 3064/3064 [00:24<00:00, 124.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decompression time profile:\n",
      "  model_infer       24.2668s  ( 98.72%)\n",
      "  arith_decode       0.2179s  (  0.89%)\n",
      "  write_file         0.0019s  (  0.01%)\n",
      "  detokenize         0.0008s  (  0.00%)\n",
      "  read_file          0.0001s  (  0.00%)\n",
      "  other              0.0951s  (  0.39%)\n",
      "Speed: 441.21 B/s\n",
      "\n",
      "=== VERIFICATION ===\n",
      "✅ SUCCESS: Perfect match!\n",
      "\n",
      "=== SUMMARY ===\n",
      "Baseline Results:\n",
      "Compression Speed: 422.57 B/s\n",
      "Decompression Speed: 441.21 B/s\n",
      "Compression Ratio: 4.75x\n",
      "BPC: 1.68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test file: {TEST_PATH}\")\n",
    "\n",
    "assert os.path.exists(TEST_PATH), f\"File not found: {TEST_PATH}\"\n",
    "\n",
    "model, tokenizer = load_model()\n",
    "\n",
    "print(\"\\n=== COMPRESSION ===\")\n",
    "comp_metrics = compress_file(model, tokenizer, TEST_PATH, COMPRESSED_PATH)\n",
    "print(f\"Ratio: {comp_metrics['ratio']:.2f}x | BPC: {comp_metrics['bpc']:.2f}\")\n",
    "\n",
    "print(\"\\n=== DECOMPRESSION ===\")\n",
    "decomp_metrics = decompress_file(model, tokenizer, COMPRESSED_PATH, DECOMPRESSED_PATH)\n",
    "print(f\"Speed: {decomp_metrics['speed_bps']:.2f} B/s\")\n",
    "\n",
    "print(\"\\n=== VERIFICATION ===\")\n",
    "validate_roundtrip(TEST_PATH, DECOMPRESSED_PATH)\n",
    "\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(f\"Baseline Results:\")\n",
    "print(f\"Compression Speed: {comp_metrics['speed_bps']:.2f} B/s\")\n",
    "print(f\"Decompression Speed: {decomp_metrics['speed_bps']:.2f} B/s\")\n",
    "print(f\"Compression Ratio: {comp_metrics['ratio']:.2f}x\")\n",
    "print(f\"BPC: {comp_metrics['bpc']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-compressor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
