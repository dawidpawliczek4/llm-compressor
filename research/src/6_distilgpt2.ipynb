{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7eab887",
   "metadata": {},
   "source": [
    "# 6. DistilGPT-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4041347d",
   "metadata": {},
   "source": [
    "## 1) Importy i konfiguracja środowiska"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d5b16c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import struct\n",
    "import time\n",
    "import constriction\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "\n",
    "TEST_PATH = \"../data/canterbury_small.bin\"\n",
    "COMPRESSED_PATH = \"../out/compressed_distilgpt2.bin\"\n",
    "DECOMPRESSED_PATH = \"../out/decompressed_distilgpt2.bin\"\n",
    "\n",
    "CONTEXT_SIZE = 1024\n",
    "STRIDE = 512\n",
    "MODEL_NAME = \"distilgpt2\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "395a3762",
   "metadata": {},
   "source": [
    "## 2) Ładowanie modelu DistilGPT-2 i tokenizera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f550315",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model_name=MODEL_NAME):\n",
    "    print(f\"Loading {model_name}...\")\n",
    "    tokenizer = GPT2TokenizerFast.from_pretrained(model_name)\n",
    "    model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "    model.eval()\n",
    "    model.to(DEVICE)\n",
    "    params = sum(p.numel() for p in model.parameters())\n",
    "    print(f\"Loaded: {params:,} parameters on {DEVICE}\")\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e15989",
   "metadata": {},
   "source": [
    "## 3) Funkcje pomocnicze predykcji następnego tokenu (KV cache)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6469a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _get_next_probs(model, input_ids, past_kv=None):\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids, past_key_values=past_kv, use_cache=True)\n",
    "        logits = out.logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1).cpu().numpy().astype(np.float64)[0]\n",
    "        probs = np.clip(probs, 1e-9, 1.0)\n",
    "        probs /= probs.sum()\n",
    "    return probs, out.past_key_values\n",
    "\n",
    "\n",
    "def _reset_context(model, context_tokens):\n",
    "    input_ids = torch.tensor([context_tokens], device=DEVICE)\n",
    "    with torch.no_grad():\n",
    "        out = model(input_ids, use_cache=True)\n",
    "        logits = out.logits[:, -1, :]\n",
    "        probs = F.softmax(logits, dim=-1).cpu().numpy().astype(np.float64)[0]\n",
    "        probs = np.clip(probs, 1e-9, 1.0)\n",
    "        probs /= probs.sum()\n",
    "    return probs, out.past_key_values, len(context_tokens)\n",
    "\n",
    "\n",
    "def print_time_profile(name, timings, total_time):\n",
    "    print(f\"\\n{name} time profile:\")\n",
    "    if total_time <= 0:\n",
    "        print(\"  No timing data\")\n",
    "        return\n",
    "\n",
    "    items = sorted(timings.items(), key=lambda x: x[1], reverse=True)\n",
    "    tracked = 0.0\n",
    "    for key, value in items:\n",
    "        tracked += value\n",
    "        pct = (value / total_time) * 100\n",
    "        print(f\"  {key:<16} {value:>8.4f}s  ({pct:>6.2f}%)\")\n",
    "\n",
    "    remaining = max(total_time - tracked, 0.0)\n",
    "    if remaining > 1e-9:\n",
    "        pct = (remaining / total_time) * 100\n",
    "        print(f\"  {'other':<16} {remaining:>8.4f}s  ({pct:>6.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72e1a25",
   "metadata": {},
   "source": [
    "## 4) Kompresja pliku token po tokenie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f1ff6160",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compress_file(model, tokenizer, input_path, output_path):\n",
    "    model.eval()\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    timings = {\n",
    "        \"read_bytes\": 0.0,\n",
    "        \"tokenize\": 0.0,\n",
    "        \"model_infer\": 0.0,\n",
    "        \"arith_encode\": 0.0,\n",
    "        \"write_file\": 0.0,\n",
    "    }\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        raw_bytes = f.read()\n",
    "    timings[\"read_bytes\"] += time.perf_counter() - t0\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    text = raw_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "    tokens = tokenizer.encode(text)\n",
    "    timings[\"tokenize\"] += time.perf_counter() - t0\n",
    "\n",
    "    num_tokens = len(tokens)\n",
    "    original_size = len(raw_bytes)\n",
    "\n",
    "    if tokenizer.decode(tokens).encode(\"utf-8\") != raw_bytes:\n",
    "        print(\"⚠️ Tokenization roundtrip not perfect - minor differences possible\")\n",
    "\n",
    "    print(f\"Original: {original_size:,} bytes → {num_tokens:,} tokens\")\n",
    "\n",
    "    encoder = constriction.stream.queue.RangeEncoder()\n",
    "    bos_id = tokenizer.eos_token_id\n",
    "\n",
    "    past_kv = None\n",
    "    kv_len = 0\n",
    "    all_seen = [bos_id]\n",
    "\n",
    "    for i in tqdm(range(num_tokens), desc=\"Compressing\"):\n",
    "        t_model = time.perf_counter()\n",
    "        if kv_len >= CONTEXT_SIZE:\n",
    "            context = all_seen[-STRIDE:]\n",
    "            probs, past_kv, kv_len = _reset_context(model, context)\n",
    "        else:\n",
    "            input_ids = torch.tensor([[all_seen[-1]]], device=DEVICE)\n",
    "            probs, past_kv = _get_next_probs(model, input_ids, past_kv)\n",
    "            kv_len += 1\n",
    "        timings[\"model_infer\"] += time.perf_counter() - t_model\n",
    "\n",
    "        t_coder = time.perf_counter()\n",
    "        dist = constriction.stream.model.Categorical(probs, perfect=False)\n",
    "        encoder.encode(int(tokens[i]), dist)\n",
    "        timings[\"arith_encode\"] += time.perf_counter() - t_coder\n",
    "\n",
    "        all_seen.append(tokens[i])\n",
    "\n",
    "    compressed_bits = encoder.get_compressed()\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(struct.pack(\"<I\", original_size))\n",
    "        f.write(struct.pack(\"<I\", num_tokens))\n",
    "        f.write(compressed_bits.tobytes())\n",
    "    timings[\"write_file\"] += time.perf_counter() - t0\n",
    "\n",
    "    duration = time.perf_counter() - start_time\n",
    "    compressed_size = os.path.getsize(output_path)\n",
    "\n",
    "    print_time_profile(\"Compression\", timings, duration)\n",
    "\n",
    "    return {\n",
    "        \"time\": duration,\n",
    "        \"original_size\": original_size,\n",
    "        \"compressed_size\": compressed_size,\n",
    "        \"ratio\": original_size / compressed_size,\n",
    "        \"bpc\": (compressed_size * 8) / original_size,\n",
    "        \"speed_bps\": original_size / duration,\n",
    "        \"num_tokens\": num_tokens,\n",
    "        \"timings\": timings,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f790d0",
   "metadata": {},
   "source": [
    "## 5) Dekompresja pliku token po tokenie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb1dc69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompress_file(model, tokenizer, input_path, output_path):\n",
    "    model.eval()\n",
    "    start_time = time.perf_counter()\n",
    "\n",
    "    timings = {\n",
    "        \"read_file\": 0.0,\n",
    "        \"model_infer\": 0.0,\n",
    "        \"arith_decode\": 0.0,\n",
    "        \"detokenize\": 0.0,\n",
    "        \"write_file\": 0.0,\n",
    "    }\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    with open(input_path, \"rb\") as f:\n",
    "        original_size = struct.unpack(\"<I\", f.read(4))[0]\n",
    "        num_tokens = struct.unpack(\"<I\", f.read(4))[0]\n",
    "        bits = np.frombuffer(f.read(), dtype=np.uint32)\n",
    "    timings[\"read_file\"] += time.perf_counter() - t0\n",
    "\n",
    "    decoder = constriction.stream.queue.RangeDecoder(bits)\n",
    "    bos_id = tokenizer.eos_token_id\n",
    "\n",
    "    print(f\"Decompressing: {num_tokens:,} tokens → {original_size:,} bytes\")\n",
    "\n",
    "    past_kv = None\n",
    "    kv_len = 0\n",
    "    all_seen = [bos_id]\n",
    "    decoded_tokens = []\n",
    "\n",
    "    for _ in tqdm(range(num_tokens), desc=\"Decompressing\"):\n",
    "        t_model = time.perf_counter()\n",
    "        if kv_len >= CONTEXT_SIZE:\n",
    "            context = all_seen[-STRIDE:]\n",
    "            probs, past_kv, kv_len = _reset_context(model, context)\n",
    "        else:\n",
    "            input_ids = torch.tensor([[all_seen[-1]]], device=DEVICE)\n",
    "            probs, past_kv = _get_next_probs(model, input_ids, past_kv)\n",
    "            kv_len += 1\n",
    "        timings[\"model_infer\"] += time.perf_counter() - t_model\n",
    "\n",
    "        t_coder = time.perf_counter()\n",
    "        dist = constriction.stream.model.Categorical(probs, perfect=False)\n",
    "        token = int(decoder.decode(dist))\n",
    "        timings[\"arith_decode\"] += time.perf_counter() - t_coder\n",
    "\n",
    "        decoded_tokens.append(token)\n",
    "        all_seen.append(token)\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    text = tokenizer.decode(decoded_tokens)\n",
    "    timings[\"detokenize\"] += time.perf_counter() - t0\n",
    "\n",
    "    t0 = time.perf_counter()\n",
    "    with open(output_path, \"wb\") as f:\n",
    "        f.write(text.encode(\"utf-8\"))\n",
    "    timings[\"write_file\"] += time.perf_counter() - t0\n",
    "\n",
    "    duration = time.perf_counter() - start_time\n",
    "    print_time_profile(\"Decompression\", timings, duration)\n",
    "\n",
    "    return {\n",
    "        \"time\": duration,\n",
    "        \"speed_bps\": original_size / duration,\n",
    "        \"timings\": timings,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba588997",
   "metadata": {},
   "source": [
    "## 6) Walidacja zgodności danych po roundtrip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d1c68c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_roundtrip(original_path, decoded_path):\n",
    "    with open(original_path, \"rb\") as f1, open(decoded_path, \"rb\") as f2:\n",
    "        orig_data = f1.read()\n",
    "        dec_data = f2.read()\n",
    "\n",
    "    if orig_data == dec_data:\n",
    "        print(\"✅ SUCCESS: Perfect match!\")\n",
    "        return True\n",
    "\n",
    "    print(\"❌ MISMATCH!\")\n",
    "    print(f\"   Original: {len(orig_data)} bytes, Decompressed: {len(dec_data)} bytes\")\n",
    "    for i in range(min(len(orig_data), len(dec_data))):\n",
    "        if orig_data[i] != dec_data[i]:\n",
    "            print(f\"   First diff at byte {i}: {orig_data[i]} vs {dec_data[i]}\")\n",
    "            break\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd67c1a8",
   "metadata": {},
   "source": [
    "## 8) Komórka uruchomieniowa pipeline z raportem metryk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d6eaacbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test file: ../data/canterbury_small.bin\n",
      "Loading distilgpt2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1a5636502774a6c8c3c6eceab2e2968",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (3064 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 81,912,576 parameters on cpu\n",
      "\n",
      "=== COMPRESSION ===\n",
      "Original: 10,846 bytes → 3,064 tokens\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58314f5b654f46cb8ae2805ac681657e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compressing:   0%|          | 0/3064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Compression time profile:\n",
      "  model_infer       16.3094s  ( 98.12%)\n",
      "  arith_encode       0.2453s  (  1.48%)\n",
      "  tokenize           0.0043s  (  0.03%)\n",
      "  write_file         0.0014s  (  0.01%)\n",
      "  read_bytes         0.0005s  (  0.00%)\n",
      "  other              0.0615s  (  0.37%)\n",
      "Ratio: 4.20x | BPC: 1.90\n",
      "\n",
      "=== DECOMPRESSION ===\n",
      "Decompressing: 3,064 tokens → 10,846 bytes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebf708c570e14c02842af85097edea0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Decompressing:   0%|          | 0/3064 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decompression time profile:\n",
      "  model_infer       14.8043s  ( 98.01%)\n",
      "  arith_decode       0.2438s  (  1.61%)\n",
      "  detokenize         0.0009s  (  0.01%)\n",
      "  write_file         0.0004s  (  0.00%)\n",
      "  read_file          0.0001s  (  0.00%)\n",
      "  other              0.0554s  (  0.37%)\n",
      "Speed: 718.04 B/s\n",
      "\n",
      "=== VERIFICATION ===\n",
      "✅ SUCCESS: Perfect match!\n",
      "\n",
      "=== SUMMARY ===\n",
      "Baseline Results:\n",
      "Compression Speed: 652.49 B/s\n",
      "Decompression Speed: 718.04 B/s\n",
      "Compression Ratio: 4.20x\n",
      "BPC: 1.90\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test file: {TEST_PATH}\")\n",
    "\n",
    "assert os.path.exists(TEST_PATH), f\"File not found: {TEST_PATH}\"\n",
    "\n",
    "model, tokenizer = load_model()\n",
    "\n",
    "print(\"\\n=== COMPRESSION ===\")\n",
    "comp_metrics = compress_file(model, tokenizer, TEST_PATH, COMPRESSED_PATH)\n",
    "print(f\"Ratio: {comp_metrics['ratio']:.2f}x | BPC: {comp_metrics['bpc']:.2f}\")\n",
    "\n",
    "print(\"\\n=== DECOMPRESSION ===\")\n",
    "decomp_metrics = decompress_file(model, tokenizer, COMPRESSED_PATH, DECOMPRESSED_PATH)\n",
    "print(f\"Speed: {decomp_metrics['speed_bps']:.2f} B/s\")\n",
    "\n",
    "print(\"\\n=== VERIFICATION ===\")\n",
    "validate_roundtrip(TEST_PATH, DECOMPRESSED_PATH)\n",
    "\n",
    "print(\"\\n=== SUMMARY ===\")\n",
    "print(\"Baseline Results:\")\n",
    "print(f\"Compression Speed: {comp_metrics['speed_bps']:.2f} B/s\")\n",
    "print(f\"Decompression Speed: {decomp_metrics['speed_bps']:.2f} B/s\")\n",
    "print(f\"Compression Ratio: {comp_metrics['ratio']:.2f}x\")\n",
    "print(f\"BPC: {comp_metrics['bpc']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
